# ğŸ“ˆ Context-Aware RAG for **Interpretable Stock Analysis**

> **Bachelor Thesis:** *Developing a Context-Aware RAG-System for Interpretable Stock Analysis Using Financial Literature*

<div align="center">

![Status](https://img.shields.io/badge/status-active-brightgreen)
![Frontend](https://img.shields.io/badge/frontend-Next.js%2015.5.0-black)
![Backend](https://img.shields.io/badge/backend-FastAPI%20%2B%20Python%203.9-blue)
![License](https://img.shields.io/badge/license-Academic-lightgrey)

</div>

## ğŸš€ Project Name & One-liner

A **RAG-powered** analysis tool that ingests financial literature to explain a stockâ€™s **fundamental metrics** in an **interpretable** way.
Built for students, analysts, and researchers who need **transparent** LLM rationales tied to sources.

---

## âœ¨ Features

* âœ… **Upload financial literature** (reports, PDFs, papers)
* âœ… **RAG + LLM** analysis of a stock via **fundamental metrics**
* âœ… **Interpretable explanations** with cited context per metric
* âœ… **Frontend flow**: Landing â†’ Analysis â†’ Metric detail
* âœ… **Reset & Re-run** (reset collection, start new runs)

---

## ğŸ—‚ Project Structure (short overview)

```text
.
â”œâ”€â”€ Backend
â”‚   â”œâ”€â”€ fastapi_rag_api.py           # FastAPI entrypoint (Uvicorn)
â”‚   â””â”€â”€ rag
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ llm.py                   # LLM provider & call wrapper
â”‚       â”œâ”€â”€ metrics.py               # Metrics Retriever
â”‚       â”œâ”€â”€ pipeline.py              # End-to-end RAG pipeline
â”‚       â”œâ”€â”€ prompt_engineering.py    # Prompts generator
â”‚       â”œâ”€â”€ query_builder.py         # Query construction for RAG
â”‚       â””â”€â”€ vectordb.py              # ChromaDB interface
â”œâ”€â”€ Evaluation
â”‚   â”œâ”€â”€ Literature                   # Literature for RAG
â”‚   â”œâ”€â”€ QA                           # Q/A for benchmarking
â”‚   â”œâ”€â”€ Results                      # Model comparisons
â”‚   â”‚   â”œâ”€â”€ gemma-7b
â”‚   â”‚   â”œâ”€â”€ llama2-7b
â”‚   â”‚   â”œâ”€â”€ llama3-latest
â”‚   â”‚   â””â”€â”€ mistral-7b
â”‚   â”œâ”€â”€ chroma_db                    # Eval DB
â”‚   â””â”€â”€ evaluation.py                # Evaluation script
â”œâ”€â”€ frontend
â”‚   â”œâ”€â”€ public
â”‚   â”‚   â””â”€â”€ Literature               # User Uploads
â”‚   â”œâ”€â”€ src
â”‚   â”‚   â”œâ”€â”€ app
â”‚   â”‚   â”‚   â”œâ”€â”€ api                  # Next.js Route Handlers
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ last-run
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ reset-collection
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ run
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ submit-literature
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ upload-literature
â”‚   â”‚   â”‚   â”œâ”€â”€ globals.css
â”‚   â”‚   â”‚   â”œâ”€â”€ layout.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ page.tsx
â”‚   â”‚   â”‚   â””â”€â”€ results
â”‚   â”‚   â”‚       â”œâ”€â”€ [metric]
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ loading.tsx
â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ not-found.tsx
â”‚   â”‚   â”‚       â”‚   â””â”€â”€ page.tsx
â”‚   â”‚   â”‚       â”œâ”€â”€ layout.tsx
â”‚   â”‚   â”‚       â””â”€â”€ page.tsx
â”‚   â”‚   â”œâ”€â”€ components
â”‚   â”‚   â”‚   â”œâ”€â”€ GradientButton.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ MetricCard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ MetricsGrid8.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ TextWindow.tsx
â”‚   â”‚   â”‚   â””â”€â”€ ui
â”‚   â”‚   â”‚       â”œâ”€â”€ CenterFooter.tsx
â”‚   â”‚   â”‚       â””â”€â”€ CopyButton.tsx
```

---

## ğŸ”§ Prerequisites

**System / Runtime**

* **Frontend:** Node.js **â‰¥ 18** (recommend: 20 LTS), **Next.js 15.5.0**
* **Backend:** **Python 3.9**, `uvicorn`, `fastapi`
* **Tools:** Git, (optional) `make`, (optional) `pnpm`/`npm`

**Package managers**

* Frontend: `pnpm` **or** `npm`
* Backend: `pip` (virtual env via `venv`)

---

## ğŸ“¦ Installation

### 1) Backend (FastAPI)

```bash
python3.9 -m venv .venv
# Linux/macOS:
source .venv/bin/activate
# Windows (PowerShell):
.venv\Scripts\Activate.ps1

pip install --upgrade pip
# If a requirements.txt exists:
pip install -r requirements.txt
````

---

### 2) Frontend (Next.js 15.5.0)

```bash
cd frontend
npm install           # or: npm install
```

---

### 3) Ollama Models

For the application to work, the following models must be downloaded in Ollama:

```bash
ollama pull llama3:latest
ollama pull mxbai-embed-large:latest
```

---

## ğŸƒ Quickstart

> Open **two terminals**â€”one for the backend, one for the frontend.

**Terminal 1 â€” start Backend**

```bash
cd Backend
source .venv/bin/activate                  # Windows: .venv\Scripts\Activate.ps1
uvicorn fastapi_rag_api:app --reload --port 8000
````

**Terminal 2 â€” start Frontend**

```bash
cd frontend
npm run dev                                   # or: pnpm dev
# App runs on http://localhost:3000
```

---

âš ï¸ **Note:** Make sure the **Ollama service is running** before starting the backend.
For example, you can start it with:

```bash
ollama serve
```

**Expected endpoints (examples)**

* `POST /run` â€” start an analysis for a ticker (used by `src/app/api/run`)
* `POST /upload-literature` â€” upload financial documents
* `POST /reset-collection` â€” reset the collection
* `GET  /last-run` â€” status / last results

*(Next.js Route Handlers under `src/app/api/*` proxy requests to the FastAPI backend.)*

---

## ğŸ–±ï¸ Usage (typical workflows)

### 1) **Landing page**

* ğŸ“¤ **Upload documents:** feed financial Literature PDFs into the vector DB (ideally fundamental analytical)
* â™»ï¸ **Reset collection** when needed
* â–¶ï¸ **Start analysis:** enter ticker (e.g., `AAPL`) â†’ **Run**

### 2) **Analysis page**

* ğŸ“Š See **all fundamental metrics** incl. current values
* ğŸ” Click a metric to open the **Metric page**

### 3) **Metric page**

* ğŸ§  **LLM explanation** for the **selected metric**, grounded with RAG sources

---

## ğŸ§ª Evaluation (brief)

* **Datasets:** `Evaluation/Literature`, `Evaluation/QA`
* **Results:** `Evaluation/Results/*` for models (`gemma-7b`, `llama2-7b`, `llama3-latest`, `mistral-7b`)
* **Script:** `Evaluation/evaluation.py` to reproduce/update outcomes


---

## ğŸ§¯ Troubleshooting

<details>
<summary><b>Frontend cannot reach Backend</b></summary>

* Confirm backend is running at <code>[http://localhost:8000](http://localhost:8000)</code>.
* CORS: does <code>ALLOWED\_ORIGINS</code> include the frontend URL?
* Check Route Handler base URL / frontend env.

</details>

<details>
<summary><b>Uploads donâ€™t appear in analysis</b></summary>

* Did you **Run** after uploading?
* Are files in the expected path (`public/Literature` or via upload endpoint)?
* Check `CHROMA_DB_DIR` and write permissions.

</details>

<details>
<summary><b>LLM errors / rate limits</b></summary>

* Is Ollama running?
* Verify model/provider settings in `rag/llm.py`.
* Tune retry/timeouts in the pipeline if needed.

</details>

---

## ğŸ“œ License

**Academic use only** within the bachelor thesis scope. For other use, please request permission first.

